{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mxhmovd/Talk2SQL/blob/main/talk2sql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo"
      ],
      "metadata": {
        "id": "B8_5zf7oAqCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkjmRTy_VQ72"
      },
      "outputs": [],
      "source": [
        "!pip install layer --upgrade -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import layer\n",
        "\n",
        "query = \"Show me the average price of wines in Italy by provinces\"\n",
        "\n",
        "model = layer.get_model('layer/t5-fine-tuning-with-layer/models/t5-english-to-sql').get_train()\n",
        "tokenizer = layer.get_model('layer/t5-fine-tuning-with-layer/models/t5-tokenizer').get_train()\n",
        "input_ids = tokenizer.encode(f\"translate English to SQL: {query}\",return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids, max_length=1024)\n",
        "tokenizer.decode(outputs[0],skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "3Im5XORrAil3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTOTXBFNKqhb"
      },
      "source": [
        "#Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwlK1RxFKIWc"
      },
      "outputs": [],
      "source": [
        "from layer.decorators import dataset, model,resources, pip_requirements, fabric\n",
        "from layer import Model\n",
        "import layer\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "layer.login()\n",
        "layer.init(\"t5-fine-tuning-with-layer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63lSAPAiWeyz"
      },
      "source": [
        "# Dataset Generation\n",
        "Unlike language to language translation datasets, we can build custom English to SQL translation pairs programmatically with the help of some templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ablj8OlgWlC2"
      },
      "outputs": [],
      "source": [
        "templates = [\n",
        "              [\"[prop1] of [nns]\",\"SELECT [prop1] FROM [nns]\"],\n",
        "              [\"[agg] [prop1] for each [breakdown]\",\"SELECT [agg]([prop1]) , [breakdown] FROM [prop1] GROUP BY [breakdown]\"],\n",
        "              [\"[prop1] of [nns] by [breakdown]\",\"SELECT [prop1] , [breakdown] FROM [nns] GROUP BY [breakdown]\"],\n",
        "              [\"[prop1] of [nns] in [location] by [breakdown]\",\"SELECT [prop1] , [breakdown] FROM [nns] WHERE location = '[location]' GROUP BY [breakdown]\"],\n",
        "              [\"[nns] having [prop1] between [number1] and [number2]\",\"SELECT name FROM [nns] WHERE [prop1] > [number1] and [prop1] < [number2]\"],\n",
        "              [\"[prop] by [breakdown]\",\"SELECT name , [breakdown] FROM [prop] GROUP BY [breakdown]\"],\n",
        "              [\"[agg] of [prop1] of [nn]\",\"SELECT [agg]([prop1]) FROM [nn]\"],\n",
        "              [\"[prop1] of [nns] before [year]\",\"SELECT [prop1] FROM [nns] WHERE date < [year]\"],\n",
        "              [\"[prop1] of [nns] after [year] in [location]\",\"SELECT [prop1] FROM [nns] WHERE date > [year] AND location='[location]'\"],\n",
        "              [\"[nns] [verb] after [year] in [location]\",\"SELECT name FROM [nns] WHERE location = '[location]' AND date > [year]\"],\n",
        "              [\"[nns] having [prop1] between [number1] and [number2] by [breakdown]\",\"SELECT name , [breakdown] FROM [nns] WHERE [prop1] < [number1] AND [prop1] > [number2] GROUP BY [breakdown]\"],\n",
        "              [\"[nns] with a [prop1] of maximum [number1] by their [breakdown]\",\"SELECT name , [breakdown] FROM [nns] WHERE [prop1] <= [number1] GROUP BY [breakdown]\"],\n",
        "              [\"[prop1] and [prop2] of [nns] since [year]\",\"SELECT [prop1] , [prop2] FROM [nns] WHERE date > [year]\"],\n",
        "              [\"[nns] which have both [prop1] and [prop2]\",\"SELECT name FROM [nns] WHERE [prop1] IS true AND [prop2] IS true\"],\n",
        "              [\"Top [number1] [nns] by [prop1]\",\"SELECT name FROM [nns] ORDER BY [prop1] DESC LIMIT [number1]\"]\n",
        "]\n",
        "template = random.choice(templates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbZ8S8m5YXNx"
      },
      "outputs": [],
      "source": [
        "objects = [\"countries\",\"wines\",\"wineries\",\"tasters\", \"provinces\",\"grapes\",\"cities\",\"bottles\",\"deliveries\"]\n",
        "object_single = [\"country\",\"wine\",\"winery\",\"taster\", \"province\",\"grape\",\"city\",\"bottle\", \"delivery\"]\n",
        "properties = [\"points\",\"price\",\"taste\",\"title\",\"texture\",\"age\",\"duration\",\"acidity\",\"flavor\",\"level\"]\n",
        "aggs = [[\"average\",\"avg\"], [\"total\",\"sum\"],[\"count\",\"count\"], [\"minimum\",\"min\"], [\"maximum\",\"max\"]]\n",
        "breakdowns = [\"quality\",\"price\",\"province\",\"country\",\"point\", \"variety\",\"flavor\",\"age\"]\n",
        "locations = [\"Italy\",\"US\",\"Portugal\",\"Spain\",\"Chile\",\"Turkey\",\"Canada\"]\n",
        "verbs = [\"produced\",\"bottled\"]\n",
        "\n",
        "regex = r\"\\[([a-z0-9]*)\\]\"\n",
        "number_of_samples = 2500\n",
        "\n",
        "@dataset(\"english_sql_translations\")\n",
        "def build_dataset():\n",
        "    rows = []\n",
        "    for index in range(0,number_of_samples):\n",
        "        template = random.choice(templates)\n",
        "        nl = template[0]\n",
        "        sql = template[1]\n",
        "\n",
        "        matches = re.finditer(regex, nl, re.MULTILINE)\n",
        "\n",
        "        for matchNum, match in enumerate(matches, start=1):\n",
        "            key = match.group()\n",
        "            prop = None\n",
        "            prop_sql = None\n",
        "            if key.startswith(\"[prop\"):\n",
        "                prop = random.choice(properties)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key in [\"[nns]\"]:\n",
        "                prop = random.choice(objects)\n",
        "                prop_sql = prop\n",
        "            if key in [\"[nn]\"]:\n",
        "                prop = random.choice(object_single)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key == \"[breakdown]\":\n",
        "                prop = random.choice(breakdowns)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key == \"[verb]\":\n",
        "                prop = random.choice(verbs)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key == \"[agg]\":\n",
        "                aggregation = random.choice(aggs)\n",
        "                prop = aggregation[0]\n",
        "                prop_sql = aggregation[1]\n",
        "            if key == \"[location]\":\n",
        "                prop = random.choice(locations)\n",
        "                prop_sql = prop\n",
        "            if key.startswith(\"[number\"):\n",
        "                prop = str(random.randint(1,1000))\n",
        "                prop_sql = prop\n",
        "            if key.startswith(\"[year\"):\n",
        "                prop = str(random.randint(1950,2022))\n",
        "                prop_sql = prop\n",
        "            \n",
        "\n",
        "            if prop is not None:\n",
        "                nl = nl.replace(key,prop)\n",
        "                sql = sql.replace(key,prop_sql)\n",
        "        \n",
        "        prefix = random.randint(1,20)\n",
        "        if prefix == 1:\n",
        "            nl = \"Show me \"+nl\n",
        "        elif prefix == 2:\n",
        "            nl = \"List \"+nl\n",
        "        elif prefix == 3:\n",
        "            nl = \"List of \"+nl\n",
        "        elif prefix == 4:\n",
        "            nl = \"Find \"+nl\n",
        "        rows.append([nl,sql])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"query\", \"sql\"])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BEjcCk_MJ95"
      },
      "source": [
        "## Register dataset to Layer\n",
        "\n",
        "In the above cell, we have used a special decorator called `@dataset` which tells Layer that our function creates dataset. Now we are going to pass this function to Layer to be run on Layer infra and register the built dataset under our project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdhJXS-YLmtH"
      },
      "outputs": [],
      "source": [
        "layer.run([build_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhDn_iW5NLyt"
      },
      "source": [
        "# Fine Tune T5\n",
        "\n",
        "Our dataset is ready and registered to Layer. Now we are going to develop the fine tuning logic, decorate the function with `@model` and pass it to Layer so that it can be run on Layer infra and registered under our project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUxkIObxoh8j"
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    import torch\n",
        "\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        step = (epoch * len(loader)) + _\n",
        "        layer.log({\"loss\": float(loss)}, step)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqA2YcZ8NdKS"
      },
      "source": [
        "Here we use 3 seperate Layer decorators:\n",
        "- [`@model`](https://docs.app.layer.ai/docs/sdk-library/model-decorator): Tells Layer that this function trains an ML model\n",
        "- [`@fabric`](https://docs.app.layer.ai/docs/sdk-library/fabric-decorator): Tells Layer the computation resources (cpu, gpu etc.) needed to train the model. Here is a list of the [available fabrics](https://docs.app.layer.ai/docs/reference/fabrics) you can use.\n",
        "- [`@pip_requirements`](https://docs.app.layer.ai/docs/sdk-library/pip-requirements-decorator): Tells the pypi libraries needed to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQYIvpAFoo2-"
      },
      "outputs": [],
      "source": [
        "@model(\"t5-tokenizer\")\n",
        "@fabric(\"f-medium\")\n",
        "@pip_requirements(packages=[\"torch\",\"transformers\",\"sentencepiece\"])\n",
        "def build_tokenizer():\n",
        "    from transformers import T5Tokenizer\n",
        "    # Load tokenizer from Hugging face\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    return tokenizer\n",
        "\n",
        "@model(\"t5-english-to-sql\", dependencies=[Model(\"t5-tokenizer\")])\n",
        "@fabric(\"f-gpu-small\")\n",
        "@pip_requirements(packages=[\"torch\",\"transformers\",\"sentencepiece\"])\n",
        "def build_model():\n",
        "    from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "    import torch.nn.functional as F\n",
        "    from torch import cuda\n",
        "    import torch\n",
        "    \n",
        "    parameters={\n",
        "        \"BATCH_SIZE\":8,          \n",
        "        \"EPOCHS\":3,              \n",
        "        \"LEARNING_RATE\":2e-05,          \n",
        "        \"MAX_SOURCE_TEXT_LENGTH\":75,   \n",
        "        \"MAX_TARGET_TEXT_LENGTH\":75,\n",
        "        \"SEED\": 42\n",
        "    }\n",
        "\n",
        "    # Log parameters to Layer\n",
        "    layer.log(parameters)\n",
        "    \n",
        "    # Set seeds for reproducibility\n",
        "    torch.manual_seed(parameters[\"SEED\"])\n",
        "    np.random.seed(parameters[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load tokenizer from Layer\n",
        "    tokenizer = layer.get_model(\"layer/t5-fine-tuning-with-layer/models/t5-tokenizer\").get_train()\n",
        "\n",
        "    # Load pretrained model from Hugging face\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    \n",
        "    def transform(row):\n",
        "        source_text = row.query\n",
        "        target_text = row.sql\n",
        "\n",
        "        source_text = ' '.join(source_text.split())\n",
        "        target_text = ' '.join(target_text.split())\n",
        "\n",
        "        source = tokenizer.batch_encode_plus([source_text], max_length= 75, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "        target = tokenizer.batch_encode_plus([target_text], max_length= 75, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    train_loader = layer.get_dataset(\"layer/t5-fine-tuning-with-layer/datasets/english_sql_translations\").to_pytorch(transform, batch_size=parameters[\"BATCH_SIZE\"], shuffle=True, num_workers=0)\n",
        "\n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=parameters[\"LEARNING_RATE\"])\n",
        "\n",
        "    for epoch in range(parameters[\"EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, device, train_loader, optimizer)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFWl17gYpOA-"
      },
      "outputs": [],
      "source": [
        "# You can train your model locally by just calling the function to debug your code.\n",
        "build_tokenizer()\n",
        "build_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "talk2sql.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}